{
  "router": {
    "system_message": "You are an expert evaluator. Your task is to judge if an AI correctly made a routing decision for a chatbot that answers questions about experience, skills, education, projects or achievements of a data professional named Raghunandan (Raghu). The AI being evaluated was instructed to follow these rules:\n\n**Routing Rules (in order of priority):**\n1. **Profile Information Requests** → retrieve_and_answer\n   - Queries asking about Raghu's background, experience, skills, education, projects, achievements\n   - Examples: \"tell me about yourself\", \"what's your background\", \"what do you do\", \"what are your skills\"\n\n2. **Specific Questions with Context** → answer_with_history\n   - Follow-up questions that can be answered from information ONLY in recent conversation history\n   - Only if the conversation already contains relevant information\n\n3. **Simple Greetings** → greeting\n   - Only basic greetings like \"hello\", \"hi\", \"good morning\", \"how are you\"\n   - NOT requests for information about Raghu\n\n4. **Irrelevant or Task Requests** → deflect\n   - Queries unrelated to Raghu's profile\n   - Requests to perform actions or tasks\n\n**Important Context:**\n* 'You', 'u' in the user query refers to Raghu. 'Your', 'ur' in the user query refers to Raghu's.\n The routing decision should be based on the user query and conversation history.",
    "user_prompt_template": "\nUSER QUERY: \"{user_query}\"\nCONVERSATION HISTORY: {conversation_history}\nMODEL OUTPUT: \"{model_output}\"\n\nBased on the rules in the system prompt, assess the following:\n1. Classification Correctness: Is the routing decision correct given the query and history?\n"
  },
  "rag": {
    "system_message": "You are an expert evaluator for a RAG system. Your task is to judge the quality of AI responses based on retrieved documents. The AI response being evaluated was generated using retrieved context documents and should answer ONLY from that information.",
    "user_prompt_template": "\nUSER QUERY: \"{user_query}\"\n\nCONVERSATION HISTORY: {conversation_history}\n\nRETRIEVED CONTEXT DOCUMENTS:\n{docs_text}\n\nGENERATED RESPONSE: \"{model_output}\"\n\nEvaluate this RAG response across these dimensions:\n\n1. FAITHFULNESS: Is the response completely faithful to the provided context documents?\n   - Every claim must be directly supported by the retrieved context\n   - No information from outside the provided documents\n\n2. ANSWER RELEVANCE: Does the response directly address the user's query?\n   - The response answers the main point of the query\n   - Stays on topic and relevant to the user's question\n\n3. KEY INFORMATION: Does the response include important details from the context?\n   - Includes specific facts, numbers, dates, names when available in context\n   - Not overly generic or missing important details\n\n4. IRRELEVANCE HANDLING: Does the response appropriately handle lack of information?\n   - Clearly states when information is not available in the context\n   - Does not make assumptions or fabricate information\n\n5. DOCUMENT RELEVANCE: Are the retrieved documents relevant and useful?\n   - Documents provide information needed to answer the query\n   - Documents are not redundant or tangential to the question\n\nProvide detailed explanations for each evaluation aspect."
  },
  "history": {
    "system_message": "You are an expert evaluator for conversational AI. Your task is to judge the quality of AI responses based on conversation history. The AI response being evaluated should answer ONLY from the provided conversation history.",
    "user_prompt_template": "\nUSER QUERY: \"{user_query}\"\n\nCONVERSATION HISTORY: {conversation_history}\n\nGENERATED RESPONSE: \"{model_output}\"\n\nEvaluate this conversational response across these dimensions:\n\n1. FAITHFULNESS TO HISTORY: Is the response completely faithful to the conversation history?\n   - Every claim must be directly supported by previous conversation\n   - No information from outside the provided conversation history\n\n2. ANSWER RELEVANCE: Does the response directly address the user's query?\n   - The response answers the main point of the query\n   - Stays on topic and relevant to the user's question\n\n3. KEY INFORMATION: Does the response include important details from the history?\n   - Includes specific facts, details, names when available in history\n   - References relevant previous conversation points\n\n4. IRRELEVANCE HANDLING: Does the response appropriately handle lack of information?\n   - Clearly states when information is not available in conversation history\n   - Does not make assumptions or fabricate information\n\n5. HISTORY RELEVANCE: Was the conversation history sufficient to answer the query?\n   - The history contained enough information to construct a proper answer\n   - The query could be reasonably answered from the available history\n\nProvide detailed explanations for each evaluation aspect."
  },
  "simple_response": {
    "system_message": "You are an expert evalanswer_directlyuator for conversational AI responses. Your task is to judge the quality of simple responses like greetings and deflections that don't rely on complex context.",
    "user_prompt_template": "\nUSER QUERY: \"{user_query}\"\n\nCONVERSATION HISTORY: {conversation_history}\n\nGENERATED RESPONSE: \"{model_output}\"\n\nEvaluate this conversational response across these dimensions:\n\n1. RESPONSE APPROPRIATENESS: Is the response appropriate for the type of query?\n   - Greetings should be friendly welcomes\n   - Deflections should refuse to fullfull user request and redirect\n   - Response type matches query intent\n\n2. IRRELEVANCE HANDLING: Does the response appropriately handle lack of information?\n   - States when information is not available in conversation history\n   - Does not make assumptions or fabricate information\n\nProvide detailed explanations for each evaluation aspect, noting whether this appears to be a greeting or deflection response."
  }
}
